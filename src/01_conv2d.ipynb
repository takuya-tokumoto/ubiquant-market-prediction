{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cdac2c",
   "metadata": {
    "papermill": {
     "duration": 0.013243,
     "end_time": "2022-04-11T11:47:51.980834",
     "exception": false,
     "start_time": "2022-04-11T11:47:51.967591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- https://www.kaggle.com/code/pythonash/end-to-end-simple-and-powerful-dnn-with-leakyrelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9abf05",
   "metadata": {
    "papermill": {
     "duration": 0.011827,
     "end_time": "2022-04-11T11:47:52.006378",
     "exception": false,
     "start_time": "2022-04-11T11:47:51.994551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# If you copy this notebook, please upvote !!\n",
    "\n",
    "##  Market Prediction with Conv2d\n",
    "It is assumed that there is a complex relationship between the features.\n",
    "\n",
    "The relationship is clarified by spatial analysis.\n",
    "\n",
    "LB = 0.152\n",
    "\n",
    "Derived from Conv1d ver. The difference in performance from previous version is under verification.\n",
    "### Refrence\n",
    "Special thanks @Lonnie Ubiquant Market Prediction with DNN\n",
    "\n",
    "- https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn## \n",
    "\n",
    "Conv1d version is here\n",
    "- https://www.kaggle.com/code/shigeeeru/prediction-including-spatial-info-with-conv1d\n",
    "\n",
    "### Note\n",
    "- delete investment feature\n",
    "- add random.set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da42ab9",
   "metadata": {
    "papermill": {
     "duration": 5.408361,
     "end_time": "2022-04-11T11:47:57.428361",
     "exception": false,
     "start_time": "2022-04-11T11:47:52.020000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from scipy import stats\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "### add random seed\n",
    "tf.random.set_seed(3)\n",
    "# tf.random.set_random_seed(3)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from model import model_conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d569f",
   "metadata": {
    "papermill": {
     "duration": 0.011505,
     "end_time": "2022-04-11T11:47:57.451967",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.440462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration\n",
    "\n",
    "If you want to train model, change 'is_trainig' to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd171882",
   "metadata": {
    "papermill": {
     "duration": 0.017844,
     "end_time": "2022-04-11T11:47:57.481372",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.463528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "#     is_training = False\n",
    "    is_training = True\n",
    "    tf_record_dataset_path = \"../data/input/CombinatorialPurgedGroupKFold_tf_record/\"\n",
    "    ### change here ###\n",
    "    output_dataset_path = \"../data/model_wight/ump-conv2d-fold5-outputs/\"\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ede702-019d-41e5-b131-9a4e5d91ac4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575a56f2",
   "metadata": {
    "papermill": {
     "duration": 0.011399,
     "end_time": "2022-04-11T11:47:57.504370",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.492971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create an IntegerLookup layer for investment_id input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497f2e7c",
   "metadata": {
    "papermill": {
     "duration": 2.401015,
     "end_time": "2022-04-11T11:47:59.917034",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.516019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 578 ms, sys: 296 ms, total: 874 ms\n",
      "Wall time: 879 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 21:48:41.074944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.084558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.085226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.086322: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-02 21:48:41.086689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.087310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.087877: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.720312: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.720988: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.721558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-02 21:48:41.722107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13795 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "investment_ids = pd.read_csv(config.tf_record_dataset_path + \"investment_ids.csv\")\n",
    "investment_id_size = len(investment_ids) + 1\n",
    "with tf.device(\"cpu\"):\n",
    "    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "    investment_id_lookup_layer.adapt(investment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed407e2",
   "metadata": {
    "papermill": {
     "duration": 0.01211,
     "end_time": "2022-04-11T11:47:59.942600",
     "exception": false,
     "start_time": "2022-04-11T11:47:59.930490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make Tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfd90fc",
   "metadata": {
    "papermill": {
     "duration": 0.02212,
     "end_time": "2022-04-11T11:47:59.976911",
     "exception": false,
     "start_time": "2022-04-11T11:47:59.954791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_function(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "      # Schema\n",
    "      {\n",
    "          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n",
    "          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n",
    "      }\n",
    "  )\n",
    "# def preprocess(item):\n",
    "#     return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\n",
    "def preprocess(item):\n",
    "    return (item[\"features\"]), item[\"target\"]\n",
    "def make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n",
    "    ds = tf.data.TFRecordDataset(file_paths)\n",
    "    ds = ds.map(decode_function)\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(batch_size * 4)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff90349",
   "metadata": {
    "papermill": {
     "duration": 0.012105,
     "end_time": "2022-04-11T11:48:00.001152",
     "exception": false,
     "start_time": "2022-04-11T11:47:59.989047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling\n",
    "\n",
    "I use layers.Conv1d. \n",
    "\n",
    "[source is here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c237d575",
   "metadata": {
    "papermill": {
     "duration": 0.034832,
     "end_time": "2022-04-11T11:48:00.072886",
     "exception": false,
     "start_time": "2022-04-11T11:48:00.038054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    ###    不偏分散にしたら？？   ###\n",
    "    \n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "def get_model():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## Dense 1 ##\n",
    "    feature_x = layers.Dense(256, activation='swish', kernel_initializer = 'he_normal')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "#     ## Dense 2 ##\n",
    "#     feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "#     feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "\n",
    "    ## convolution2D 1 ##\n",
    "    feature_x = layers.Reshape((64,64,1))(feature_x)\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=1, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution2D 2 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution2D 3 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "    ## Dense 3 ##\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\", kernel_initializer = 'he_normal')(feature_x)\n",
    "    ## Dense 4 ##\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 5 ##    \n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\", kernel_initializer = 'he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 6 ##\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\", kernel_initializer = 'he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 7 ##\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    \n",
    "    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = 0.001,\n",
    "        decay_steps = 9700,\n",
    "        decay_rate = 0.98\n",
    "    )\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n",
    "    \n",
    "    model.compile(optimizer = adam, loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17aaa7",
   "metadata": {
    "papermill": {
     "duration": 0.014013,
     "end_time": "2022-04-11T11:48:00.099701",
     "exception": false,
     "start_time": "2022-04-11T11:48:00.085688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take a look at this Model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2696e156",
   "metadata": {
    "papermill": {
     "duration": 1.758654,
     "end_time": "2022-04-11T11:48:01.872309",
     "exception": false,
     "start_time": "2022-04-11T11:48:00.113655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311050f6",
   "metadata": {
    "papermill": {
     "duration": 0.031834,
     "end_time": "2022-04-11T11:48:01.937050",
     "exception": false,
     "start_time": "2022-04-11T11:48:01.905216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2c7059",
   "metadata": {
    "papermill": {
     "duration": 13400.553823,
     "end_time": "2022-04-11T15:31:22.587349",
     "exception": false,
     "start_time": "2022-04-11T11:48:02.033526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 300), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-02 21:50:22.850677: E tensorflow/stream_executor/cuda/cuda_dnn.cc:361] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2022-05-02 21:50:22.851490: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_ops.cc:1120 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_1/conv1d_3/Conv1D' defined at (most recent call last):\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3029, in _run_cell\n      return runner(coro)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3472, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5030/2972595981.py\", line 1, in <module>\n      get_ipython().run_cell_magic('time', '', 'models = []\\nfor i in range(5):\\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\\n    print(valid_ds)\\n    model = get_model()\\n    if config.is_training:\\n        train_ds = make_dataset([train_path])\\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\\n        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\\n        model.save_weights(f\"model_{i}.tf\")\\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\\n            plt.title(metric.upper())\\n            plt.show()\\n    else:\\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\\n    y_vals = []\\n    for _, y in valid_ds:\\n        y_vals += list(y.numpy().reshape(-1))\\n    y_val = np.array(y_vals)\\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\\n    models.append(model)\\n    print(f\"Pearson Score: {pearson_score}\")\\n')\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2472, in run_cell_magic\n      result = fn(*args, **kwargs)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/decorator.py\", line 232, in fun\n      return caller(func, *(extras + args), **kw)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n      call = lambda f, *a, **k: f(*a, **k)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magics/execution.py\", line 1335, in time\n      exec(code, glob, local_ns)\n    File \"<timed exec>\", line 12, in <module>\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 248, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 240, in convolution_op\n      name=self.__class__.__name__)\nNode: 'model_1/conv1d_3/Conv1D'\nDNN library is not found.\n\t [[{{node model_1/conv1d_3/Conv1D}}]] [Op:__inference_train_function_4427]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'model_1/conv1d_3/Conv1D' defined at (most recent call last):\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 387, in do_execute\n      cell_id=cell_id,\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      raw_cell, store_history, silent, shell_futures, cell_id\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3029, in _run_cell\n      return runner(coro)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3472, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3552, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_5030/2972595981.py\", line 1, in <module>\n      get_ipython().run_cell_magic('time', '', 'models = []\\nfor i in range(5):\\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\\n    print(valid_ds)\\n    model = get_model()\\n    if config.is_training:\\n        train_ds = make_dataset([train_path])\\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\\n        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\\n        model.save_weights(f\"model_{i}.tf\")\\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\\n            plt.title(metric.upper())\\n            plt.show()\\n    else:\\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\\n    y_vals = []\\n    for _, y in valid_ds:\\n        y_vals += list(y.numpy().reshape(-1))\\n    y_val = np.array(y_vals)\\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\\n    models.append(model)\\n    print(f\"Pearson Score: {pearson_score}\")\\n')\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2472, in run_cell_magic\n      result = fn(*args, **kwargs)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/decorator.py\", line 232, in fun\n      return caller(func, *(extras + args), **kw)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n      call = lambda f, *a, **k: f(*a, **k)\n    File \"/home/ubuntu/anaconda3/envs/py37/lib/python3.7/site-packages/IPython/core/magics/execution.py\", line 1335, in time\n      exec(code, glob, local_ns)\n    File \"<timed exec>\", line 12, in <module>\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 248, in call\n      outputs = self.convolution_op(inputs, self.kernel)\n    File \"/home/ubuntu/.local/lib/python3.7/site-packages/keras/layers/convolutional.py\", line 240, in convolution_op\n      name=self.__class__.__name__)\nNode: 'model_1/conv1d_3/Conv1D'\nDNN library is not found.\n\t [[{{node model_1/conv1d_3/Conv1D}}]] [Op:__inference_train_function_4427]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = []\n",
    "for i in range(5):\n",
    "    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n",
    "    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n",
    "    valid_ds = make_dataset([valid_path], mode=\"valid\")\n",
    "    print(valid_ds)\n",
    "    model = get_model()\n",
    "    if config.is_training:\n",
    "        train_ds = make_dataset([train_path])\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n",
    "        early_stop = keras.callbacks.EarlyStopping(patience=10)\n",
    "        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n",
    "        model.save_weights(f\"model_{i}.tf\")\n",
    "        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n",
    "            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n",
    "            plt.title(metric.upper())\n",
    "            plt.show()\n",
    "    else:\n",
    "        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n",
    "    y_vals = []\n",
    "    for _, y in valid_ds:\n",
    "        y_vals += list(y.numpy().reshape(-1))\n",
    "    y_val = np.array(y_vals)\n",
    "    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n",
    "    models.append(model)\n",
    "    print(f\"Pearson Score: {pearson_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29135eee",
   "metadata": {
    "papermill": {
     "duration": 11.752445,
     "end_time": "2022-04-11T15:31:46.553360",
     "exception": false,
     "start_time": "2022-04-11T15:31:34.800915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa167bd",
   "metadata": {
    "papermill": {
     "duration": 11.822004,
     "end_time": "2022-04-11T15:32:10.713744",
     "exception": false,
     "start_time": "2022-04-11T15:31:58.891740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def preprocess_test(investment_id, feature):\n",
    "#     return (investment_id, feature), 0\n",
    "\n",
    "# def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "#     ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "#     ds = ds.map(preprocess_test)\n",
    "#     ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "#     return ds\n",
    "\n",
    "def make_test_dataset(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d30e60",
   "metadata": {
    "papermill": {
     "duration": 13.910633,
     "end_time": "2022-04-11T15:32:36.464883",
     "exception": false,
     "start_time": "2022-04-11T15:32:22.554250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test() \n",
    "features = [f\"f_{i}\" for i in range(300)]\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    ds = make_test_dataset(test_df[features])\n",
    "    sample_prediction_df['target'] = inference(models, ds)\n",
    "    env.predict(sample_prediction_df)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca6101",
   "metadata": {
    "papermill": {
     "duration": 12.31513,
     "end_time": "2022-04-11T15:33:00.671575",
     "exception": false,
     "start_time": "2022-04-11T15:32:48.356445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13532.203975,
   "end_time": "2022-04-11T15:33:16.188561",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-11T11:47:43.984586",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
