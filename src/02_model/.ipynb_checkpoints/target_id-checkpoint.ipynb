{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f77d2bb",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/datasets/lonnieqin/ump-combinatorialpurgedgroupkfold-tf-record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cdac2c",
   "metadata": {
    "papermill": {
     "duration": 0.013243,
     "end_time": "2022-04-11T11:47:51.980834",
     "exception": false,
     "start_time": "2022-04-11T11:47:51.967591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- https://www.kaggle.com/code/pythonash/end-to-end-simple-and-powerful-dnn-with-leakyrelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9abf05",
   "metadata": {
    "papermill": {
     "duration": 0.011827,
     "end_time": "2022-04-11T11:47:52.006378",
     "exception": false,
     "start_time": "2022-04-11T11:47:51.994551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# If you copy this notebook, please upvote !!\n",
    "\n",
    "##  Market Prediction with Conv2d\n",
    "It is assumed that there is a complex relationship between the features.\n",
    "\n",
    "The relationship is clarified by spatial analysis.\n",
    "\n",
    "LB = 0.152\n",
    "\n",
    "Derived from Conv1d ver. The difference in performance from previous version is under verification.\n",
    "### Refrence\n",
    "Special thanks @Lonnie Ubiquant Market Prediction with DNN\n",
    "\n",
    "- https://www.kaggle.com/lonnieqin/ubiquant-market-prediction-with-dnn## \n",
    "\n",
    "Conv1d version is here\n",
    "- https://www.kaggle.com/code/shigeeeru/prediction-including-spatial-info-with-conv1d\n",
    "\n",
    "### Note\n",
    "- delete investment feature\n",
    "- add random.set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da42ab9",
   "metadata": {
    "papermill": {
     "duration": 5.408361,
     "end_time": "2022-04-11T11:47:57.428361",
     "exception": false,
     "start_time": "2022-04-11T11:47:52.020000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 03:54:10.997608: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:\n",
      "2022-05-04 03:54:10.997636: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from scipy import stats\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "### add random seed\n",
    "tf.random.set_seed(3)\n",
    "# tf.random.set_random_seed(3)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from model import model_conv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d569f",
   "metadata": {
    "papermill": {
     "duration": 0.011505,
     "end_time": "2022-04-11T11:47:57.451967",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.440462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configuration\n",
    "\n",
    "If you want to train model, change 'is_trainig' to 'True'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd171882",
   "metadata": {
    "papermill": {
     "duration": 0.017844,
     "end_time": "2022-04-11T11:47:57.481372",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.463528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "#     is_training = False\n",
    "    is_training = True\n",
    "    tf_record_dataset_path = \"../data/input/CombinatorialPurgedGroupKFold_tf_record/\"\n",
    "    ### change here ###\n",
    "    output_dataset_path = \"../data/model_wight/ump-conv2d-fold5-outputs/\"\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ede702-019d-41e5-b131-9a4e5d91ac4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575a56f2",
   "metadata": {
    "papermill": {
     "duration": 0.011399,
     "end_time": "2022-04-11T11:47:57.504370",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.492971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create an IntegerLookup layer for investment_id input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497f2e7c",
   "metadata": {
    "papermill": {
     "duration": 2.401015,
     "end_time": "2022-04-11T11:47:59.917034",
     "exception": false,
     "start_time": "2022-04-11T11:47:57.516019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/input/CombinatorialPurgedGroupKFold_tf_record/investment_ids.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/input/CombinatorialPurgedGroupKFold_tf_record/investment_ids.csv'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "investment_ids = pd.read_csv(config.tf_record_dataset_path + \"investment_ids.csv\")\n",
    "investment_id_size = len(investment_ids) + 1\n",
    "with tf.device(\"cpu\"):\n",
    "    investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "    investment_id_lookup_layer.adapt(investment_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed407e2",
   "metadata": {
    "papermill": {
     "duration": 0.01211,
     "end_time": "2022-04-11T11:47:59.942600",
     "exception": false,
     "start_time": "2022-04-11T11:47:59.930490",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Make Tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dfd90fc",
   "metadata": {
    "papermill": {
     "duration": 0.02212,
     "end_time": "2022-04-11T11:47:59.976911",
     "exception": false,
     "start_time": "2022-04-11T11:47:59.954791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_function(record_bytes):\n",
    "    return tf.io.parse_single_example(\n",
    "      # Data\n",
    "      record_bytes,\n",
    "      # Schema\n",
    "      {\n",
    "          \"features\": tf.io.FixedLenFeature([300], dtype=tf.float32),\n",
    "          \"time_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"investment_id\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          \"target\": tf.io.FixedLenFeature([], dtype=tf.float32)\n",
    "      }\n",
    "  )\n",
    "# def preprocess(item):\n",
    "#     return (item[\"investment_id\"], item[\"features\"]), item[\"target\"]\n",
    "def preprocess(item):\n",
    "    return (item[\"features\"]), item[\"target\"]\n",
    "def make_dataset(file_paths, batch_size=4096, mode=\"train\"):\n",
    "    ds = tf.data.TFRecordDataset(file_paths)\n",
    "    ds = ds.map(decode_function)\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(batch_size * 4)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff90349",
   "metadata": {
    "papermill": {
     "duration": 0.012105,
     "end_time": "2022-04-11T11:48:00.001152",
     "exception": false,
     "start_time": "2022-04-11T11:47:59.989047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Modeling\n",
    "\n",
    "I use layers.Conv1d. \n",
    "\n",
    "[source is here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c237d575",
   "metadata": {
    "papermill": {
     "duration": 0.034832,
     "end_time": "2022-04-11T11:48:00.072886",
     "exception": false,
     "start_time": "2022-04-11T11:48:00.038054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correlation(x, y, axis=-2):\n",
    "    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    y = math_ops.cast(y, x.dtype)\n",
    "    n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "    xsum = tf.reduce_sum(x, axis=axis)\n",
    "    ysum = tf.reduce_sum(y, axis=axis)\n",
    "    xmean = xsum / n\n",
    "    ymean = ysum / n\n",
    "    ###    不偏分散にしたら？？   ###\n",
    "    \n",
    "    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "\n",
    "    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "    corr = cov / tf.sqrt(xvar * yvar)\n",
    "    return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "def get_model():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "    \n",
    "    ## Dense 1 ##\n",
    "    feature_x = layers.Dense(256, activation='swish', kernel_initializer = 'he_normal')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "#     ## Dense 2 ##\n",
    "#     feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "#     feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "\n",
    "    ## convolution2D 1 ##\n",
    "    feature_x = layers.Reshape((64,64,1))(feature_x)\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=1, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution2D 2 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "    ## convolution2D 3 ##\n",
    "    feature_x = layers.Conv2D(filters=32, kernel_size=4, strides=4, padding='same', kernel_initializer = 'he_normal')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU(0.5)(feature_x)\n",
    "\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "    ## Dense 3 ##\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\", kernel_initializer = 'he_normal')(feature_x)\n",
    "    ## Dense 4 ##\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 5 ##    \n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\", kernel_initializer = 'he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 6 ##\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\", kernel_initializer = 'he_normal')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    ## Dense 7 ##\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    \n",
    "    learning_sch = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate = 0.001,\n",
    "        decay_steps = 9700,\n",
    "        decay_rate = 0.98\n",
    "    )\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate = learning_sch)\n",
    "    \n",
    "    model.compile(optimizer = adam, loss='mse', metrics=['mse', \"mae\", \"mape\", rmse, correlation])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17aaa7",
   "metadata": {
    "papermill": {
     "duration": 0.014013,
     "end_time": "2022-04-11T11:48:00.099701",
     "exception": false,
     "start_time": "2022-04-11T11:48:00.085688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take a look at this Model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2696e156",
   "metadata": {
    "papermill": {
     "duration": 1.758654,
     "end_time": "2022-04-11T11:48:01.872309",
     "exception": false,
     "start_time": "2022-04-11T11:48:00.113655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 03:54:46.653484: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/lib:/usr/lib:\n",
      "2022-05-04 03:54:46.653515: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-04 03:54:46.653537: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-17-131): /proc/driver/nvidia/version does not exist\n",
      "2022-05-04 03:54:46.653718: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# model.summary()\n",
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311050f6",
   "metadata": {
    "papermill": {
     "duration": 0.031834,
     "end_time": "2022-04-11T11:48:01.937050",
     "exception": false,
     "start_time": "2022-04-11T11:48:01.905216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2c7059",
   "metadata": {
    "papermill": {
     "duration": 13400.553823,
     "end_time": "2022-04-11T15:31:22.587349",
     "exception": false,
     "start_time": "2022-04-11T11:48:02.033526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 300), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1418/2972595981.py\", line 1, in <cell line: 1>\n      get_ipython().run_cell_magic('time', '', 'models = []\\nfor i in range(5):\\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\\n    print(valid_ds)\\n    model = get_model()\\n    if config.is_training:\\n        train_ds = make_dataset([train_path])\\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\\n        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\\n        model.save_weights(f\"model_{i}.tf\")\\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\\n            plt.title(metric.upper())\\n            plt.show()\\n    else:\\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\\n    y_vals = []\\n    for _, y in valid_ds:\\n        y_vals += list(y.numpy().reshape(-1))\\n    y_val = np.array(y_vals)\\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\\n    models.append(model)\\n    print(f\"Pearson Score: {pearson_score}\")\\n')\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2347, in run_cell_magic\n      result = fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/magics/execution.py\", line 1316, in time\n      exec(code, glob, local_ns)\n    File \"<timed exec>\", line 12, in <module>\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1009, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n../data/input/CombinatorialPurgedGroupKFold_tf_record/fold_0_train.tfrecords; No such file or directory\n\t [[{{node IteratorGetNext}}]] [Op:__inference_train_function_4083]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'IteratorGetNext' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 504, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 493, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 724, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_1418/2972595981.py\", line 1, in <cell line: 1>\n      get_ipython().run_cell_magic('time', '', 'models = []\\nfor i in range(5):\\n    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\\n    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\\n    valid_ds = make_dataset([valid_path], mode=\"valid\")\\n    print(valid_ds)\\n    model = get_model()\\n    if config.is_training:\\n        train_ds = make_dataset([train_path])\\n        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\\n        early_stop = keras.callbacks.EarlyStopping(patience=10)\\n        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\\n        model.save_weights(f\"model_{i}.tf\")\\n        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\\n            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\\n            plt.title(metric.upper())\\n            plt.show()\\n    else:\\n        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\\n    y_vals = []\\n    for _, y in valid_ds:\\n        y_vals += list(y.numpy().reshape(-1))\\n    y_val = np.array(y_vals)\\n    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\\n    models.append(model)\\n    print(f\"Pearson Score: {pearson_score}\")\\n')\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2347, in run_cell_magic\n      result = fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/magics/execution.py\", line 1316, in time\n      exec(code, glob, local_ns)\n    File \"<timed exec>\", line 12, in <module>\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1009, in step_function\n      data = next(iterator)\nNode: 'IteratorGetNext'\n../data/input/CombinatorialPurgedGroupKFold_tf_record/fold_0_train.tfrecords; No such file or directory\n\t [[{{node IteratorGetNext}}]] [Op:__inference_train_function_4083]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "models = []\n",
    "for i in range(5):\n",
    "    train_path = f\"{config.tf_record_dataset_path}fold_{i}_train.tfrecords\"\n",
    "    valid_path = f\"{config.tf_record_dataset_path}fold_{i}_test.tfrecords\"\n",
    "    valid_ds = make_dataset([valid_path], mode=\"valid\")\n",
    "    print(valid_ds)\n",
    "    model = get_model()\n",
    "    if config.is_training:\n",
    "        train_ds = make_dataset([train_path])\n",
    "        checkpoint = keras.callbacks.ModelCheckpoint(f\"model_{i}.tf\", monitor=\"val_correlation\", mode=\"min\", save_best_only=True, save_weights_only=True)\n",
    "        early_stop = keras.callbacks.EarlyStopping(patience=10)\n",
    "        history = model.fit(train_ds, epochs=50, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n",
    "        model.save_weights(f\"model_{i}.tf\")\n",
    "        for metric in [\"loss\", \"mae\", \"mape\", \"rmse\", \"correlation\"]:\n",
    "            pd.DataFrame(history.history, columns=[metric, f\"val_{metric}\"]).plot()\n",
    "            plt.title(metric.upper())\n",
    "            plt.show()\n",
    "    else:\n",
    "        model.load_weights(f\"{config.output_dataset_path}model_{i}.tf\")\n",
    "    y_vals = []\n",
    "    for _, y in valid_ds:\n",
    "        y_vals += list(y.numpy().reshape(-1))\n",
    "    y_val = np.array(y_vals)\n",
    "    pearson_score = stats.pearsonr(model.predict(valid_ds).reshape(-1), y_val)[0]\n",
    "    models.append(model)\n",
    "    print(f\"Pearson Score: {pearson_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29135eee",
   "metadata": {
    "papermill": {
     "duration": 11.752445,
     "end_time": "2022-04-11T15:31:46.553360",
     "exception": false,
     "start_time": "2022-04-11T15:31:34.800915",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa167bd",
   "metadata": {
    "papermill": {
     "duration": 11.822004,
     "end_time": "2022-04-11T15:32:10.713744",
     "exception": false,
     "start_time": "2022-04-11T15:31:58.891740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def preprocess_test(investment_id, feature):\n",
    "#     return (investment_id, feature), 0\n",
    "\n",
    "# def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "#     ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "#     ds = ds.map(preprocess_test)\n",
    "#     ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "#     return ds\n",
    "\n",
    "def make_test_dataset(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d30e60",
   "metadata": {
    "papermill": {
     "duration": 13.910633,
     "end_time": "2022-04-11T15:32:36.464883",
     "exception": false,
     "start_time": "2022-04-11T15:32:22.554250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ubiquant\n",
    "env = ubiquant.make_env()\n",
    "iter_test = env.iter_test() \n",
    "features = [f\"f_{i}\" for i in range(300)]\n",
    "for (test_df, sample_prediction_df) in iter_test:\n",
    "    ds = make_test_dataset(test_df[features])\n",
    "    sample_prediction_df['target'] = inference(models, ds)\n",
    "    env.predict(sample_prediction_df)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca6101",
   "metadata": {
    "papermill": {
     "duration": 12.31513,
     "end_time": "2022-04-11T15:33:00.671575",
     "exception": false,
     "start_time": "2022-04-11T15:32:48.356445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13532.203975,
   "end_time": "2022-04-11T15:33:16.188561",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-11T11:47:43.984586",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
